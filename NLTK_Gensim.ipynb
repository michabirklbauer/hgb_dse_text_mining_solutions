{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d49afba-1ae8-4b99-adfb-4e3f0789bbac",
   "metadata": {},
   "source": [
    "### Loading our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95502191-b13c-4fea-bbcc-69f9d29e5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# english data\n",
    "classes_en = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tech\"}\n",
    "train_en = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/AGNews/train.csv\", \n",
    "                       names = [\"Label\", \"Title\", \"Article\"],\n",
    "                       encoding = \"utf-8\")\n",
    "test_en = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/AGNews/test.csv\", \n",
    "                      names = [\"Label\", \"Title\", \"Article\"],\n",
    "                      encoding = \"utf-8\")\n",
    "\n",
    "# german data\n",
    "train_de = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/10kGNAD/train.csv\", \n",
    "                       sep = \";\", names = [\"Label\", \"Article\"], \n",
    "                       quotechar = \"\\'\", quoting = csv.QUOTE_MINIMAL, encoding = \"utf-8\")\n",
    "test_de = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/10kGNAD/test.csv\", \n",
    "                       sep = \";\", names = [\"Label\", \"Article\"], \n",
    "                       quotechar = \"\\'\", quoting = csv.QUOTE_MINIMAL, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13421461-7335-4c46-9b9f-2d4e134eea2a",
   "metadata": {},
   "source": [
    "By iterating over the dataframe columns we can construct a \"vanilla\" list of documents that we can work on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9e3169c-0449-41c2-9535-68b7a3f5aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_en = [classes_en[int(row[\"Label\"])] for i, row in train_en.iterrows()]\n",
    "articles_en = [row[\"Article\"] for i, row in train_en.iterrows()]\n",
    "labels_de = [row[\"Label\"] for i, row in train_de.iterrows()]\n",
    "articles_de = [row[\"Article\"] for i, row in train_de.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a3a938-d719-4300-925b-7734d9480567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
       " 'Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.',\n",
       " 'Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.',\n",
       " 'Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.',\n",
       " 'AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_en[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b7ddb-1d99-45fd-b9d5-e4e70db3e35a",
   "metadata": {},
   "source": [
    "# **NLTK**\n",
    "\n",
    "[https://www.nltk.org/](https://www.nltk.org/)\n",
    "\n",
    "NLTK - short for Natural Language Toolkit - is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning and wrappers for industrial-strength NLP libraries.\n",
    "\n",
    "I mostly use NLTK for preprocessing tasks because it is more light-weight and straight forward than spaCy in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b54b783-e6f0-4c64-890e-d303a5b8817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords as nltkStopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a51cd-3006-4373-b0ce-1e4a5db5b3ac",
   "metadata": {},
   "source": [
    "### NLTK tokenizes documents which are any string variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d409dac-d43a-43a4-ada9-554182772014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Micha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "def tokenize(doc):\n",
    "    return nltk.word_tokenize(doc)\n",
    "\n",
    "articles_en_tokenized = [tokenize(doc) for doc in articles_en]\n",
    "articles_de_tokenized = [tokenize(doc) for doc in articles_de]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb87440-321d-458e-a516-4aac1bf15688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reuters',\n",
       " '-',\n",
       " 'Short-sellers',\n",
       " ',',\n",
       " 'Wall',\n",
       " 'Street',\n",
       " \"'s\",\n",
       " 'dwindling\\\\band',\n",
       " 'of',\n",
       " 'ultra-cynics',\n",
       " ',',\n",
       " 'are',\n",
       " 'seeing',\n",
       " 'green',\n",
       " 'again',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_en_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408f4b3-ed37-4c25-b4ad-3e03e0fbedcb",
   "metadata": {},
   "source": [
    "### Stemming can be done with NLTK's Snowball Stemmer\n",
    "\n",
    "[https://www.nltk.org/api/nltk.stem.snowball.html](https://www.nltk.org/api/nltk.stem.snowball.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461d8815-e62b-4fb4-b013-d3339df493e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokenized_document, language = None):\n",
    "        stemmer = SnowballStemmer(language, ignore_stopwords = False)\n",
    "        return [stemmer.stem(word) for word in tokenized_document]\n",
    "    \n",
    "articles_en_stemmed = [stem(doc, \"english\") for doc in articles_en_tokenized]\n",
    "articles_de_stemmed = [stem(doc, \"german\") for doc in articles_de_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b328f7-046b-4916-b83f-1802cd415a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reuter',\n",
       " '-',\n",
       " 'short-sel',\n",
       " ',',\n",
       " 'wall',\n",
       " 'street',\n",
       " \"'s\",\n",
       " 'dwindling\\\\band',\n",
       " 'of',\n",
       " 'ultra-cyn',\n",
       " ',',\n",
       " 'are',\n",
       " 'see',\n",
       " 'green',\n",
       " 'again',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_en_stemmed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eeedf6-ac68-4c4a-bf40-858bbaeb98ba",
   "metadata": {},
   "source": [
    "### NLTK also offers built-in stopword sets for different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03cb5fa9-d4f6-4924-8edc-ef0644013585",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = set(nltkStopwords.words(\"english\"))\n",
    "stopwords_de = set(nltkStopwords.words(\"german\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf13239-77d4-4fbd-87bb-0bf83e39b2a6",
   "metadata": {},
   "source": [
    "### The english stopwords are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "952154b1-b41f-450a-a956-324666630d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"me,that'll,myself,this,hasn,nor,below,what,during,to,in,again,s,mightn,is,haven't,any,didn't,other,about,mustn,her,being,couldn,won,hasn't,an,by,shan't,for,until,doesn,before,so,ain,hers,just,few,can,hadn't,while,who,your,yourself,it's,been,herself,hadn,them,both,you've,wasn't,i,my,you'd,his,ourselves,on,not,that,between,t,does,we,out,themselves,because,off,have,and,y,same,whom,haven,couldn't,above,wasn,was,up,having,the,at,itself,theirs,after,most,isn't,here,m,no,they,doesn't,needn't,she's,ve,which,am,you,you'll,their,ours,a,into,re,doing,how,wouldn,against,why,should,there,were,our,don,where,he,if,of,under,wouldn't,those,its,be,these,it,or,you're,ma,shan,more,ll,yourselves,o,once,aren't,himself,too,then,as,mustn't,had,now,won't,shouldn,some,needn,through,but,don't,didn,did,all,will,mightn't,each,weren,when,only,isn,very,d,down,weren't,with,should've,aren,such,shouldn't,yours,from,than,further,are,over,him,do,has,she,own\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(stopwords_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17cfe54-5d57-4659-b2ca-47fefd178f3d",
   "metadata": {},
   "source": [
    "### And the german ones are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa15a4a-513c-419a-a06b-f9ab145bb119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'das,einig,ich,solches,deinem,einen,dieses,manche,aber,unsere,in,dich,hatte,manchem,zum,ihm,vor,manches,seiner,oder,war,solche,seinen,wirst,haben,mancher,bei,doch,weiter,an,habe,damit,des,hatten,dies,welchen,andern,dieser,euch,einigen,so,unseren,er,kann,jeden,dann,anderes,dazu,hat,zur,sonst,wenn,dasselbe,jetzt,mein,um,solchen,zwischen,derselben,ihres,sondern,mit,allen,welche,demselben,mich,ihren,nichts,ihn,keiner,der,ander,wie,seines,keine,derselbe,ohne,meiner,nur,meinen,hin,deiner,manchen,ihnen,jenes,über,deinen,ihrer,eines,welchem,bis,diesen,durch,ein,anders,während,eure,deine,uns,unter,es,werde,eine,was,mir,wieder,dir,ins,muss,ihrem,sich,zwar,seine,wollen,bist,die,jenem,vom,weg,euer,jeder,ihr,anderm,also,zu,sind,am,alles,einigem,wir,denselben,seinem,einiger,anderer,meines,im,nun,diese,als,nach,unserem,dem,einmal,warst,aus,keinen,sehr,allem,hab,auch,solcher,hinter,soll,derer,bin,meinem,musste,anderen,einem,eurem,ist,wird,sollte,werden,gewesen,solchem,desselben,keinem,jede,alle,dein,können,einiges,von,welches,euren,noch,daß,deines,hier,andere,jene,jedem,sie,könnte,wollte,dessen,eures,und,da,dieselbe,auf,kein,welcher,aller,jener,einige,du,für,machen,ihre,jedes,weil,wo,würde,denn,will,diesem,meine,indem,einer,dass,unseres,viel,anderem,sein,anderr,gegen,man,dieselben,etwas,nicht,waren,eurer,unser,würden,ob,keines,dort,selbst,den,jenen'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(stopwords_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb452b-ba84-4c1c-8ba8-5066a2910167",
   "metadata": {},
   "source": [
    "### Removing stopwords from our stemmed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f4b0f60-11c7-4d59-b080-07e186f3238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(stemmed_document, stopwords):\n",
    "        def is_stopword(word):\n",
    "            return not word in stopwords\n",
    "        return list(filter(is_stopword, stemmed_document))\n",
    "    \n",
    "articles_en_final = [remove_stopwords(doc, stopwords_en) for doc in articles_en_stemmed]\n",
    "articles_de_final = [remove_stopwords(doc, stopwords_de) for doc in articles_de_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea6e1ce3-03f9-4c07-b5f0-8687c429ecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reuter',\n",
       " '-',\n",
       " 'short-sel',\n",
       " ',',\n",
       " 'wall',\n",
       " 'street',\n",
       " \"'s\",\n",
       " 'dwindling\\\\band',\n",
       " 'ultra-cyn',\n",
       " ',',\n",
       " 'see',\n",
       " 'green',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_en_final[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400603ab-fd34-442f-83cb-bc6fa423515e",
   "metadata": {},
   "source": [
    "# **Gensim**\n",
    "\n",
    "[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)\n",
    "\n",
    "Gensim titles itself as \"Topic Modelling for Humans\" and is the third and final NLP library that we will have a look at. I have mainly used Gensim to build TF-IDF models and run text queries on datasets. We are going to use our NLTK preprocessed documents as input to build a dictionary, corpus and index with Gensim and calculate the TF-IDF matrix to run text queries on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da13794-2d08-46dc-920c-8acbbffff714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Micha\\anaconda3\\envs\\text\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b53d3-6dcd-4b60-9639-2a112e161d7a",
   "metadata": {},
   "source": [
    "### Building the TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee423808-4843-41ef-92b5-7e930498e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 500 # adjust if model too big\n",
    "corpus_dictionary_en = corpora.Dictionary(articles_en_final[:size])\n",
    "corpus_en = [corpus_dictionary_en.doc2bow(document) for document in articles_en_final[:size]]\n",
    "model_en = models.TfidfModel(corpus_en)\n",
    "index_en = similarities.MatrixSimilarity(model_en[corpus_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3234d6-ee85-4fd2-b516-5db5a9f49bb6",
   "metadata": {},
   "source": [
    "To calculate the similarity of our input the query has to be preprocessed the same way our data was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1da0c310-f226-47f7-bb96-536865b5d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_en(query_string):\n",
    "    q = corpus_dictionary_en.doc2bow(remove_stopwords(stem(tokenize(query_string), language = \"english\"), stopwords_en))\n",
    "    q_model = model_en[q]\n",
    "    result = index_en[q_model]\n",
    "    result = sorted(enumerate(result), key = lambda item: -item[1])\n",
    "    for i, j in enumerate(result):\n",
    "        if i > 2:\n",
    "            break\n",
    "        print(j, articles_en[:size][j[0]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c3769-8a3b-4ab5-b55a-94fdb159863c",
   "metadata": {},
   "source": [
    "### Gensim returns the resulting document and its similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ac1848f-1f64-4c14-81e9-a9673b508c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237, 0.3952839) Scientists in the United States find a way to turn lazy monkeys into workaholics using gene therapy.\n",
      "(450, 0.21606433) AFP - National Basketball Association players trying to win a fourth consecutive Olympic gold medal for the United States have gotten the wake-up call that the \"Dream Team\" days are done even if supporters have not.\n",
      "(462, 0.20889904)  ATHENS (Reuters) - The United States beat Canada in a world  best time to qualify for the final of the men's Olympic eights  race Sunday, as the two crews renewed their fierce rivalry in  front of a raucous crowd at Schinias.\n"
     ]
    }
   ],
   "source": [
    "query_en(\"Scientists United States\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65b34f-6c0d-499c-b870-05605168623c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
