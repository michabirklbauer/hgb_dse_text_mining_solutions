{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d49afba-1ae8-4b99-adfb-4e3f0789bbac",
   "metadata": {},
   "source": [
    "### Loading our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95502191-b13c-4fea-bbcc-69f9d29e5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "# english data\n",
    "classes_en = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tech\"}\n",
    "train_en = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/AGNews/train.csv\", \n",
    "                       names = [\"Label\", \"Title\", \"Article\"],\n",
    "                       encoding = \"utf-8\")\n",
    "test_en = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/AGNews/test.csv\", \n",
    "                      names = [\"Label\", \"Title\", \"Article\"],\n",
    "                      encoding = \"utf-8\")\n",
    "\n",
    "# german data\n",
    "train_de = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/10kGNAD/train.csv\", \n",
    "                       sep = \";\", names = [\"Label\", \"Article\"], \n",
    "                       quotechar = \"\\'\", quoting = csv.QUOTE_MINIMAL, encoding = \"utf-8\")\n",
    "test_de = pd.read_csv(\"https://raw.githubusercontent.com/michabirklbauer/hgb_dse_text_mining/master/data/10kGNAD/test.csv\", \n",
    "                       sep = \";\", names = [\"Label\", \"Article\"], \n",
    "                       quotechar = \"\\'\", quoting = csv.QUOTE_MINIMAL, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13421461-7335-4c46-9b9f-2d4e134eea2a",
   "metadata": {},
   "source": [
    "By iterating over the dataframe columns we can construct a \"vanilla\" list of documents that we can work on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9e3169c-0449-41c2-9535-68b7a3f5aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_en = [classes_en[int(row[\"Label\"])] for i, row in train_en.iterrows()]\n",
    "articles_en = [row[\"Article\"] for i, row in train_en.iterrows()]\n",
    "labels_de = [row[\"Label\"] for i, row in train_de.iterrows()]\n",
    "articles_de = [row[\"Article\"] for i, row in train_de.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a3a938-d719-4300-925b-7734d9480567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
       " 'Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.',\n",
       " 'Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.',\n",
       " 'Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.',\n",
       " 'AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_en[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b7ddb-1d99-45fd-b9d5-e4e70db3e35a",
   "metadata": {},
   "source": [
    "# **NLTK**\n",
    "\n",
    "[https://www.nltk.org/](https://www.nltk.org/)\n",
    "\n",
    "NLTK - short for Natural Language Toolkit - is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning and wrappers for industrial-strength NLP libraries.\n",
    "\n",
    "I mostly use NLTK for preprocessing tasks because it is more light-weight and straight forward than spaCy in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b54b783-e6f0-4c64-890e-d303a5b8817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords as nltkStopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a51cd-3006-4373-b0ce-1e4a5db5b3ac",
   "metadata": {},
   "source": [
    "### NLTK tokenizes documents which are any string variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d409dac-d43a-43a4-ada9-554182772014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\P42587\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "\n",
    "articles_en_tokenized = [nltk.word_tokenize(doc) for doc in articles_en]\n",
    "articles_de_tokenized = [nltk.word_tokenize(doc) for doc in articles_de]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb87440-321d-458e-a516-4aac1bf15688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reuters',\n",
       " '-',\n",
       " 'Short-sellers',\n",
       " ',',\n",
       " 'Wall',\n",
       " 'Street',\n",
       " \"'s\",\n",
       " 'dwindling\\\\band',\n",
       " 'of',\n",
       " 'ultra-cynics',\n",
       " ',',\n",
       " 'are',\n",
       " 'seeing',\n",
       " 'green',\n",
       " 'again',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_en_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408f4b3-ed37-4c25-b4ad-3e03e0fbedcb",
   "metadata": {},
   "source": [
    "### Stemming can be done with NLTK's Snowball Stemmer\n",
    "\n",
    "[https://www.nltk.org/api/nltk.stem.snowball.html](https://www.nltk.org/api/nltk.stem.snowball.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461d8815-e62b-4fb4-b013-d3339df493e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokenized_document: str, language: str | None = None) -> List[str]:\n",
    "        stemmer = SnowballStemmer(language, ignore_stopwords = False)\n",
    "        return [stemmer.stem(word) for word in tokenized_document]\n",
    "    \n",
    "articles_en_stemmed = [stem(doc, \"english\") for doc in articles_en_tokenized]\n",
    "articles_de_stemmed = [stem(doc, \"german\") for doc in articles_de_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b328f7-046b-4916-b83f-1802cd415a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reuter',\n",
       " '-',\n",
       " 'short-sel',\n",
       " ',',\n",
       " 'wall',\n",
       " 'street',\n",
       " \"'s\",\n",
       " 'dwindling\\\\band',\n",
       " 'of',\n",
       " 'ultra-cyn',\n",
       " ',',\n",
       " 'are',\n",
       " 'see',\n",
       " 'green',\n",
       " 'again',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_en_stemmed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eeedf6-ac68-4c4a-bf40-858bbaeb98ba",
   "metadata": {},
   "source": [
    "### NLTK also offers built-in stopword sets for different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03cb5fa9-d4f6-4924-8edc-ef0644013585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\P42587\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stopwords_en = set(nltkStopwords.words(\"english\"))\n",
    "stopwords_de = set(nltkStopwords.words(\"german\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf13239-77d4-4fbd-87bb-0bf83e39b2a6",
   "metadata": {},
   "source": [
    "### The english stopwords are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "952154b1-b41f-450a-a956-324666630d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the,that,have,now,will,my,this,d,aren't,itself,t,yours,while,below,where,its,ain,which,o,haven't,wasn,each,before,her,himself,you'll,didn't,can,should,theirs,mightn't,don,mustn't,doesn't,she's,s,until,wouldn,is,out,does,because,had,too,shouldn't,being,at,isn't,against,no,ve,don't,did,haven,shan't,wouldn't,has,you're,do,of,yourself,whom,you've,some,isn,we,couldn't,won't,a,than,aren,it's,i,off,herself,m,why,weren't,needn,nor,to,hadn,an,with,how,should've,couldn,so,won,are,what,mustn,that'll,when,myself,over,mightn,our,she,as,been,not,between,it,just,above,ours,all,after,shouldn,these,here,very,ma,those,yourselves,own,any,doing,needn't,by,he,ourselves,such,re,you'd,hers,then,wasn't,for,through,from,into,in,am,under,ll,on,once,and,few,down,hadn't,your,who,hasn't,during,didn,up,same,or,more,them,other,shan,if,be,was,further,his,they,both,about,having,him,y,only,most,but,doesn,me,you,again,their,hasn,there,were,weren,themselves\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(stopwords_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17cfe54-5d57-4659-b2ca-47fefd178f3d",
   "metadata": {},
   "source": [
    "### And the german ones are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa15a4a-513c-419a-a06b-f9ab145bb119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'durch,anderr,deines,diesen,ihr,jede,keinem,haben,gewesen,manchem,würde,unserem,jeder,will,zwischen,dasselbe,viel,ihm,demselben,keinen,diese,oder,unser,zum,ihren,im,einigen,sein,derselbe,sondern,euren,über,jedes,nach,welchen,dieselbe,seines,selbst,das,ander,desselben,wirst,denselben,euch,seiner,ihrem,die,dies,etwas,allen,ihrer,soll,bin,einig,ihnen,dem,meiner,wenn,meinen,mir,kann,dann,eine,diesem,mein,anderem,derer,ihn,unseren,welchem,wo,er,machen,andere,können,auf,eurer,eurem,unsere,wollte,für,sollte,jeden,manches,seinem,dazu,eures,jener,also,nur,unter,damit,der,dessen,weiter,würden,dir,zwar,seine,bist,keine,an,noch,um,hab,uns,hinter,so,eines,ins,solche,solches,anders,auch,hin,wird,sie,von,eure,deinen,sonst,daß,unseres,wir,doch,während,welche,einiger,denn,anderen,war,nun,jenes,dich,musste,werden,nichts,als,meinem,welcher,ohne,dass,deine,sind,solchen,solcher,zur,waren,einer,ich,und,aber,deiner,hier,hatten,jenen,derselben,meines,ein,aller,jetzt,dein,jedem,jene,meine,einige,vom,warst,alle,jenem,seinen,man,du,in,keiner,am,sehr,weg,manche,einen,da,weil,ist,anderer,ob,nicht,anderm,mich,manchen,solchem,vor,ihre,gegen,sich,dieses,dieselben,keines,deinem,wieder,zu,anderes,ihres,kein,was,bei,dort,habe,hat,einiges,hatte,könnte,welches,wie,muss,mancher,euer,indem,des,aus,wollen,einigem,den,andern,einmal,es,einem,werde,alles,mit,dieser,bis,allem'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(stopwords_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb452b-ba84-4c1c-8ba8-5066a2910167",
   "metadata": {},
   "source": [
    "### Removing stopwords from our stemmed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f4b0f60-11c7-4d59-b080-07e186f3238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(stemmed_document: str, stopwords: Set) -> List[str]:\n",
    "        def is_stopword(word):\n",
    "            return not word in stopwords\n",
    "        return list(filter(is_stopword, stemmed_document))\n",
    "    \n",
    "articles_en_final = [remove_stopwords(doc, stopwords_en) for doc in articles_en_stemmed]\n",
    "articles_de_final = [remove_stopwords(doc, stopwords_de) for doc in articles_de_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea6e1ce3-03f9-4c07-b5f0-8687c429ecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reuter',\n",
       " '-',\n",
       " 'short-sel',\n",
       " ',',\n",
       " 'wall',\n",
       " 'street',\n",
       " \"'s\",\n",
       " 'dwindling\\\\band',\n",
       " 'ultra-cyn',\n",
       " ',',\n",
       " 'see',\n",
       " 'green',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_en_final[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400603ab-fd34-442f-83cb-bc6fa423515e",
   "metadata": {},
   "source": [
    "# **Gensim**\n",
    "\n",
    "[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)\n",
    "\n",
    "Gensim titles itself as \"Topic Modelling for Humans\" and is the third and final NLP library that we will have a look at. I have mainly used Gensim to build TF-IDF models and run text queries on datasets. We are going to use our NLTK preprocessed documents as input to build a dictionary, corpus and index with Gensim and calculate the TF-IDF matrix to run text queries on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da13794-2d08-46dc-920c-8acbbffff714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b53d3-6dcd-4b60-9639-2a112e161d7a",
   "metadata": {},
   "source": [
    "### Building the TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee423808-4843-41ef-92b5-7e930498e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 500 # adjust if model too big\n",
    "corpus_dictionary_en = corpora.Dictionary(articles_en_final[:size])\n",
    "corpus_en = [corpus_dictionary_en.doc2bow(document) for document in articles_en_final[:size]]\n",
    "model_en = models.TfidfModel(corpus_en)\n",
    "index_en = similarities.MatrixSimilarity(model_en[corpus_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3234d6-ee85-4fd2-b516-5db5a9f49bb6",
   "metadata": {},
   "source": [
    "To calculate the similarity of our input the query has to be preprocessed the same way our data was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1da0c310-f226-47f7-bb96-536865b5d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_en(query_string: str) -> List[Tuple[int, float]]:\n",
    "    q = corpus_dictionary_en.doc2bow(remove_stopwords(stem(nltk.word_tokenize(query_string), language = \"english\"), stopwords_en))\n",
    "    q_model = model_en[q]\n",
    "    result = index_en[q_model]\n",
    "    result = sorted(enumerate(result), key = lambda item: -item[1])\n",
    "    for i, j in enumerate(result):\n",
    "        if i > 2:\n",
    "            break\n",
    "        print(j, articles_en[:size][j[0]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c3769-8a3b-4ab5-b55a-94fdb159863c",
   "metadata": {},
   "source": [
    "### Gensim returns the resulting document and its similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ac1848f-1f64-4c14-81e9-a9673b508c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237, 0.3952839) Scientists in the United States find a way to turn lazy monkeys into workaholics using gene therapy.\n",
      "(450, 0.21606433) AFP - National Basketball Association players trying to win a fourth consecutive Olympic gold medal for the United States have gotten the wake-up call that the \"Dream Team\" days are done even if supporters have not.\n",
      "(462, 0.20889904)  ATHENS (Reuters) - The United States beat Canada in a world  best time to qualify for the final of the men's Olympic eights  race Sunday, as the two crews renewed their fierce rivalry in  front of a raucous crowd at Schinias.\n"
     ]
    }
   ],
   "source": [
    "query_en(\"Scientists United States\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65b34f-6c0d-499c-b870-05605168623c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
